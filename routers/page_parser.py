# malody_api/routers/page_parser.py
from fastapi import APIRouter, Query, HTTPException
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import subprocess
import json
import os
import re
import asyncio
import aiohttp
from bs4 import BeautifulSoup

from malody_api.core.database import get_db_connection
from malody_api.core.models import APIResponse
from malody_api.utils.crawler_manager import crawler_manager

router = APIRouter(prefix="/page-parser", tags=["page-parser"])

# å†·å´æ—¶é—´è·Ÿè¸ªï¼ˆå†…å­˜ä¸­ï¼‰
crawler_cooldown = {}

# è°±é¢çŠ¶æ€æ˜ å°„
STATUS_MAP = {
    "Stable": 2,
    "Beta": 1, 
    "Alpha": 0
}

# æ¨¡å¼æ˜ å°„
MODE_MAP = {
    0: "Key",
    1: "Step", 
    2: "DJ",
    3: "Catch",
    4: "Pad", 
    5: "Taiko",
    6: "Ring",
    7: "Slide",
    8: "Live",
    9: "Cube"
}

# åŠ è½½Modæ˜ å°„é…ç½®ï¼ˆå ä½ç¬¦ï¼‰
MOD_MAPPING = {}
try:
    mod_mapping_path = os.path.join(os.path.dirname(__file__), '..', 'mod_mapping.json')
    with open(mod_mapping_path, 'r', encoding='utf-8') as f:
        MOD_MAPPING = json.load(f)
    print(f"âœ… æˆåŠŸåŠ è½½Modæ˜ å°„ï¼Œå…±{len(MOD_MAPPING)}ä¸ªMod")
except Exception as e:
    print(f"âŒ åŠ è½½Modæ˜ å°„å¤±è´¥: {e}ï¼Œä½¿ç”¨å ä½ç¬¦æ˜ å°„")
    # åˆ›å»ºå ä½ç¬¦æ˜ å°„
    MOD_MAPPING = {f"g_mod_{i}": f"mod_{i}" for i in range(20)}

class PageParserService:
    """é¡µé¢è§£ææœåŠ¡ - ä¸“æ³¨äºæ’è¡Œæ¦œè§£æ"""
    
    def __init__(self):
        self.base_url = "https://m.mugzone.net"
        self.session = None
    
    async def get_session(self):
        """è·å–aiohttpä¼šè¯"""
        if self.session is None:
            timeout = aiohttp.ClientTimeout(total=30)
            self.session = aiohttp.ClientSession(timeout=timeout)
        return self.session
    
    async def close_session(self):
        """å…³é—­ä¼šè¯"""
        if self.session:
            await self.session.close()
            self.session = None
    
    async def parse_chart_page(self, cid: int) -> Dict[str, Any]:
        """è§£æè°±é¢é¡µé¢ - ä¸“æ³¨äºæ’è¡Œæ¦œæ•°æ®"""
        session = await self.get_session()
        url = f"{self.base_url}/chart/{cid}"
        
        try:
            async with session.get(url) as response:
                if response.status == 404:
                    return {"error": f"è°±é¢ä¸å­˜åœ¨: {cid}"}
                elif response.status != 200:
                    return {"error": f"é¡µé¢è·å–å¤±è´¥: {response.status}"}
                
                html = await response.text()
                return await self._parse_chart_html(html, cid)
                
        except aiohttp.ClientError as e:
            return {"error": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"}
        except Exception as e:
            return {"error": f"è§£æå¤±è´¥: {str(e)}"}
        finally:
            await self.close_session()
    
    async def _parse_chart_html(self, html: str, cid: int) -> Dict[str, Any]:
        """è§£æè°±é¢é¡µé¢HTML - ä¸“æ³¨äºæ’è¡Œæ¦œæ•°æ®"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # è§£æåŸºç¡€è°±é¢ä¿¡æ¯ï¼ˆä»…ç”¨äºä¸Šä¸‹æ–‡ï¼‰
        chart_info = await self._parse_basic_chart_info(soup, cid)
        
        # è§£ææ’è¡Œæ¦œæ•°æ®ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰
        ranking_data = await self._parse_ranking_data(soup)
        
        return {
            "chart_info": chart_info,
            "ranking": ranking_data,
            "parsed_at": datetime.now().isoformat(),
            "total_rankings": len(ranking_data)
        }
    
    async def _parse_basic_chart_info(self, soup, cid: int) -> Dict[str, Any]:
        """è§£æåŸºç¡€è°±é¢ä¿¡æ¯ï¼ˆä»…ç”¨äºä¸Šä¸‹æ–‡ï¼‰"""
        song_title_div = soup.find('div', class_='song_title')
        if not song_title_div:
            return {"cid": cid, "error": "æ— æ³•æ‰¾åˆ°è°±é¢ä¿¡æ¯"}
        
        # åªæå–æœ€åŸºç¡€çš„ä¿¡æ¯ç”¨äºä¸Šä¸‹æ–‡
        right_div = song_title_div.find('div', class_='right')
        title_h3 = right_div.find('h3', class_='textfix') if right_div else None
        title_h2 = right_div.find('h2', class_='textfix title') if right_div else None
        
        title_en = title_h3.get_text(strip=True) if title_h3 else None
        title_jp = title_h2.get_text(strip=True) if title_h2 else None
        
        # æå–SIDï¼ˆç”¨äºå…³è”æ•°æ®åº“ï¼‰
        sub_h2 = right_div.find('h2', class_='sub') if right_div else None
        sid = None
        if sub_h2:
            sub_text = sub_h2.get_text()
            id_match = re.search(r'ID:c?(\d+)', sub_text)
            sid = int(id_match.group(1)) if id_match else None
        
        return {
            "cid": cid,
            "sid": sid,
            "title_en": title_en,
            "title_jp": title_jp
        }
    
    async def _parse_ranking_data(self, soup) -> List[Dict[str, Any]]:
        """è§£ææ’è¡Œæ¦œæ•°æ® - æ ¸å¿ƒåŠŸèƒ½"""
        ranking_list = soup.find('ul', class_='list')
        if not ranking_list:
            return []
        
        rankings = []
        current_judge = self._get_current_judge(soup)
        
        for item in ranking_list.find_all('li'):
            ranking_data = await self._parse_ranking_item(item, current_judge)
            if ranking_data:
                rankings.append(ranking_data)
        
        return rankings
    
    def _get_current_judge(self, soup) -> str:
        """è·å–å½“å‰åˆ¤å®šéš¾åº¦"""
        judge_select = soup.find('select', id='g_judge')
        if judge_select:
            selected_option = judge_select.find('option', selected=True)
            if selected_option:
                return selected_option.get_text(strip=True)
        return "All"
    
    async def _parse_ranking_item(self, item, judge: str) -> Dict[str, Any]:
        """è§£æå•ä¸ªæ’è¡Œæ¦œé¡¹ç›®"""
        try:
            # æ’åä¿¡æ¯
            rank = self._parse_rank(item)
            
            # ç©å®¶ä¿¡æ¯
            player_info = self._parse_player_info(item)
            
            # åˆ†æ•°å’Œç»Ÿè®¡ä¿¡æ¯
            score_info = self._parse_score_info(item)
            
            # å‡†ç¡®ç‡ä¿¡æ¯
            accuracy_info = self._parse_accuracy_info(item)
            
            # Modä¿¡æ¯
            mods = self._parse_mods(item)
            
            # æ—¶é—´ä¿¡æ¯
            achieved_time = self._parse_time_info(item)
            
            # æ‰“å‡»ç»Ÿè®¡
            hit_stats = self._parse_hit_stats(item)
            
            return {
                "rank": rank,
                "player": player_info,
                **score_info,
                **accuracy_info,
                "mods": mods,
                "achieved_time": achieved_time,
                "judge": judge,
                "hit_stats": hit_stats
            }
        except Exception as e:
            print(f"è§£ææ’è¡Œæ¦œé¡¹ç›®å¤±è´¥: {e}")
            return None
    
    def _parse_rank(self, item) -> Optional[int]:
        """è§£ææ’å"""
        rank_label = item.find('i', class_=re.compile('label top-'))
        if not rank_label:
            return None
        
        class_name = ' '.join(rank_label.get('class', []))
        rank_match = re.search(r'top-(\d+)', class_name)
        return int(rank_match.group(1)) if rank_match else None
    
    def _parse_player_info(self, item) -> Dict[str, Any]:
        """è§£æç©å®¶ä¿¡æ¯"""
        rank_span = item.find('span', class_='rank')
        rank_img = rank_span.find('img') if rank_span else None
        
        name_span = item.find('span', class_='name')
        name_link = name_span.find('a') if name_span else None
        
        player_uid = self._extract_uid_from_url(name_link.get('href', '')) if name_link else None
        player_name = name_link.get_text(strip=True) if name_link else None
        player_avatar = rank_img.get('src') if rank_img else None
        
        return {
            "uid": player_uid,
            "name": player_name,
            "avatar": player_avatar
        }
    
    def _parse_score_info(self, item) -> Dict[str, Any]:
        """è§£æåˆ†æ•°ä¿¡æ¯"""
        score_span = item.find('span', class_='score')
        combo_span = item.find('span', class_='combo')
        
        score = None
        combo = None
        
        if score_span and score_span.get_text(strip=True):
            try:
                score = int(score_span.get_text(strip=True))
            except ValueError:
                pass
        
        if combo_span and combo_span.get_text(strip=True):
            try:
                combo = int(combo_span.get_text(strip=True))
            except ValueError:
                pass
        
        return {
            "score": score,
            "combo": combo
        }
    
    def _parse_accuracy_info(self, item) -> Dict[str, Any]:
        """è§£æå‡†ç¡®ç‡ä¿¡æ¯"""
        acc_span = item.find('span', class_='acc')
        if not acc_span:
            return {}
        
        acc_round = acc_span.find('i', class_='g_round')
        acc_text = acc_span.find('em')
        
        accuracy_percent = None
        accuracy_width = None
        accuracy_color = None
        
        if acc_round:
            style = acc_round.get('style', '')
            if 'width:' in style:
                accuracy_width = style.split('width:')[-1].split(';')[0].strip()
            accuracy_color = ' '.join([c for c in acc_round.get('class', []) if c.startswith('color-')])
        
        if acc_text:
            accuracy_text = acc_text.get_text(strip=True).replace('%', '')
            try:
                accuracy_percent = float(accuracy_text)
            except ValueError:
                pass
        
        return {
            "accuracy": accuracy_percent,
            "accuracy_percentage": f"{accuracy_percent}%" if accuracy_percent else None,
            "accuracy_width": accuracy_width,
            "accuracy_color": accuracy_color
        }
    
    def _parse_mods(self, item) -> List[str]:
        """è§£æMods - ä½¿ç”¨å ä½ç¬¦æ˜ å°„"""
        mod_span = item.find('span', class_='mod')
        if not mod_span:
            return []
        
        mods = []
        mod_icons = mod_span.find_all('i', class_=re.compile('g_mod'))
        for icon in mod_icons:
            mod_class = ' '.join(icon.get('class', []))
            # ä½¿ç”¨å ä½ç¬¦æ˜ å°„
            mod_name = MOD_MAPPING.get(mod_class, mod_class)
            mods.append(mod_name)
        
        return mods
    
    def _parse_time_info(self, item) -> Optional[str]:
        """è§£ææ—¶é—´ä¿¡æ¯"""
        time_span = item.find('span', class_='time')
        return time_span.get_text(strip=True) if time_span else None
    
    def _parse_hit_stats(self, item) -> Dict[str, int]:
        """è§£ææ‰“å‡»ç»Ÿè®¡"""
        title = item.get('title', '')
        if not title or '/' not in title:
            return {"perfect": 0, "good": 0, "miss": 0, "unknown": 0}
        
        parts = title.split('/')
        return {
            "perfect": int(parts[0]) if parts[0] else 0,
            "good": int(parts[1]) if len(parts) > 1 and parts[1] else 0,
            "miss": int(parts[2]) if len(parts) > 2 and parts[2] else 0,
            "unknown": int(parts[3]) if len(parts) > 3 and parts[3] else 0
        }
    
    def _extract_uid_from_url(self, url: str) -> Optional[int]:
        """ä»URLä¸­æå–UID"""
        if not url:
            return None
        match = re.search(r'/accounts/user/(\d+)', url)
        return int(match.group(1)) if match else None

@router.get("/chart/{cid}", response_model=APIResponse)
async def parse_chart_page(cid: int):
    """è§£æè°±é¢é¡µé¢ - ä¸“æ³¨äºæ’è¡Œæ¦œæ•°æ®"""
    try:
        # æ£€æŸ¥å†·å´æ—¶é—´
        current_time = datetime.now()
        if cid in crawler_cooldown:
            last_update = crawler_cooldown[cid]
            if current_time - last_update < timedelta(minutes=5):
                return APIResponse(
                    success=False,
                    error=f"CID {cid} æœ€è¿‘å·²è§£æï¼Œè¯·5åˆ†é’Ÿåå†è¯•",
                    timestamp=current_time
                )
        
        service = PageParserService()
        result = await service.parse_chart_page(cid)
        
        if "error" in result:
            raise HTTPException(status_code=404, detail=result["error"])
        
        # æ›´æ–°å†·å´æ—¶é—´
        crawler_cooldown[cid] = current_time
        
        return APIResponse(
            success=True,
            data=result,
            message=f"æˆåŠŸè§£æè°±é¢æ’è¡Œæ¦œï¼Œæ‰¾åˆ° {result.get('total_rankings', 0)} æ¡è®°å½•",
            timestamp=current_time
        )
        
    except HTTPException:
        raise
    except Exception as e:
        return APIResponse(
            success=False,
            error=str(e),
            timestamp=datetime.now()
        )

@router.get("/chart/{cid}/ranking")
async def get_chart_ranking_only(cid: int):
    """ä»…è·å–è°±é¢æ’è¡Œæ¦œæ•°æ®ï¼ˆä¸åŒ…å«è°±é¢ä¿¡æ¯ï¼‰"""
    try:
        service = PageParserService()
        result = await service.parse_chart_page(cid)
        
        if "error" in result:
            raise HTTPException(status_code=404, detail=result["error"])
        
        # åªè¿”å›æ’è¡Œæ¦œæ•°æ®
        ranking_data = {
            "ranking": result.get("ranking", []),
            "total_rankings": result.get("total_rankings", 0),
            "parsed_at": result.get("parsed_at")
        }
        
        return APIResponse(
            success=True,
            data=ranking_data,
            message=f"æˆåŠŸè·å–æ’è¡Œæ¦œæ•°æ®ï¼Œå…± {ranking_data['total_rankings']} æ¡è®°å½•",
            timestamp=datetime.now()
        )
        
    except HTTPException:
        raise
    except Exception as e:
        return APIResponse(
            success=False,
            error=str(e),
            timestamp=datetime.now()
        )

# ä¿ç•™å…¶ä»–å¿…è¦çš„APIç«¯ç‚¹ï¼ˆæœç´¢æ­Œæ›²ã€è·å–æ­Œæ›²è¯¦æƒ…ç­‰ï¼‰
@router.get("/song/search", response_model=APIResponse)
async def search_songs(
    query: str = Query(..., description="æ­Œæ›²åæˆ–è‰ºæœ¯å®¶å"),
    limit: int = Query(20, description="è¿”å›æ•°é‡", ge=1, le=50)
):
    """æœç´¢æ­Œæ›²ï¼ˆä»æ•°æ®åº“ï¼‰"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        search_query = f"%{query}%"
        cursor.execute("""
            SELECT s.sid, s.title, s.artist, 
                   COUNT(DISTINCT c.cid) as chart_count,
                   COUNT(DISTINCT CASE WHEN c.status = 2 THEN c.cid END) as stable_count,
                   GROUP_CONCAT(DISTINCT c.mode) as modes
            FROM songs s
            LEFT JOIN charts c ON s.sid = c.sid
            WHERE s.title LIKE ? OR s.artist LIKE ?
            GROUP BY s.sid, s.title, s.artist
            ORDER BY stable_count DESC, chart_count DESC
            LIMIT ?
        """, (search_query, search_query, limit))
        
        results = cursor.fetchall()
        conn.close()
        
        if not results:
            return APIResponse(
                success=False,
                error=f"æœªæ‰¾åˆ°åŒ¹é…çš„æ­Œæ›²: {query}",
                timestamp=datetime.now()
            )
        
        songs = [
            {
                "sid": row[0],
                "title": row[1],
                "artist": row[2],
                "chart_count": row[3],
                "stable_count": row[4],
                "modes": [int(m) for m in row[5].split(',')] if row[5] else []
            } for row in results
        ]
        
        return APIResponse(
            success=True,
            data=songs,
            message=f"æ‰¾åˆ° {len(songs)} ä¸ªåŒ¹é…çš„æ­Œæ›²",
            timestamp=datetime.now()
        )
        
    except Exception as e:
        return APIResponse(
            success=False,
            error=str(e),
            timestamp=datetime.now()
        )

async def trigger_sid_update(sid: int) -> Dict[str, Any]:
    """è§¦å‘SIDæ›´æ–°"""
    try:
        current_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        crawler_script = os.path.join(current_dir, "stb_crawler.py")
        
        if not os.path.exists(crawler_script):
            return {
                "success": False,
                "error": f"çˆ¬è™«è„šæœ¬ä¸å­˜åœ¨: {crawler_script}"
            }
        
        cmd = [
            "python", crawler_script,
            "--sid", str(sid),
            "--log-level", "INFO",
            "--skip-test"
        ]
        
        print(f"ğŸš€ æ‰§è¡Œçˆ¬è™«å‘½ä»¤: {' '.join(cmd)}")
        
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            cwd=current_dir
        )
        
        try:
            stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=600)
        except asyncio.TimeoutError:
            process.terminate()
            return {
                "success": False,
                "error": "çˆ¬è™«æ‰§è¡Œè¶…æ—¶ï¼ˆ10åˆ†é’Ÿï¼‰"
            }
        
        result = {
            "success": process.returncode == 0,
            "stdout": stdout.decode('utf-8', errors='ignore') if stdout else "",
            "stderr": stderr.decode('utf-8', errors='ignore') if stderr else "",
            "returncode": process.returncode
        }
        
        if result["success"]:
            print(f"âœ… SID {sid} çˆ¬å–æˆåŠŸ")
        else:
            print(f"âŒ SID {sid} çˆ¬å–å¤±è´¥: {result['stderr'][:200]}")
        
        return result
        
    except Exception as e:
        error_msg = f"æ‰§è¡Œçˆ¬è™«æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        print(f"âŒ {error_msg}")
        return {
            "success": False,
            "error": error_msg
        }